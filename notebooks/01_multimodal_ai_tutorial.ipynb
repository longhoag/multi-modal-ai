{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caf5c5cd",
   "metadata": {},
   "source": [
    "# Multi-Modal AI Application: Complete Tutorial\n",
    "## Week 2 Final Project - Social Media Content Moderation\n",
    "\n",
    "This comprehensive notebook demonstrates the implementation of a multi-modal AI system that combines text, image, and tabular data for automated content moderation. We'll build a complete pipeline from data preprocessing to model deployment.\n",
    "\n",
    "### Project Overview\n",
    "- **Use Case**: Social Media Content Moderation\n",
    "- **Modalities**: Text (posts, comments), Images (photos, memes), Tabular (user metadata, engagement metrics)\n",
    "- **Goal**: Automated content safety classification with explainable AI\n",
    "\n",
    "### Learning Objectives\n",
    "1. Multi-modal data preprocessing and augmentation\n",
    "2. Individual encoder architectures (Transformers, CNNs, MLPs)\n",
    "3. Advanced fusion strategies\n",
    "4. Model training and optimization\n",
    "5. Evaluation and interpretation\n",
    "6. Deployment considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd26d8d6",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f81d2c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package Version: 0.1.0\n",
      "Author: Student\n",
      "Email: student@example.com\n",
      "‚úÖ Package info loaded successfully!\n",
      "‚úÖ NumPy version: 1.26.4\n",
      "‚úÖ Pandas version: 2.2.3\n",
      "‚ö†Ô∏è  PyTorch not available - install with: pip install torch\n",
      "\n",
      "üéØ Environment Status:\n",
      "   Python: 3.13.0\n",
      "   Working Directory: /Volumes/deuxSSD/Developer/multi-modal-ai/notebooks\n",
      "   PyTorch Available: False\n",
      "   Ready for demo mode: ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# Check Package Version and Information\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our package info from the __init__.py file\n",
    "try:\n",
    "    import src\n",
    "    print(f\"Package Version: {src.__version__}\")\n",
    "    print(f\"Author: {src.__author__}\")\n",
    "    print(f\"Email: {src.__email__}\")\n",
    "except ImportError:\n",
    "    # Alternative method to get package info\n",
    "    sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "    exec(open('../src/__init__.py').read())\n",
    "    print(f\"Package Version: {__version__}\")\n",
    "    print(f\"Author: {__author__}\")\n",
    "    print(f\"Email: {__email__}\")\n",
    "\n",
    "print(\"‚úÖ Package info loaded successfully!\")\n",
    "\n",
    "# Check for available libraries (basic ones we have installed)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(f\"‚úÖ NumPy version: {np.__version__}\")\n",
    "print(f\"‚úÖ Pandas version: {pd.__version__}\")\n",
    "\n",
    "# Check for PyTorch (optional)\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "    pytorch_available = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  PyTorch not available - install with: pip install torch\")\n",
    "    pytorch_available = False\n",
    "\n",
    "print(f\"\\nüéØ Environment Status:\")\n",
    "print(f\"   Python: {sys.version.split()[0]}\")\n",
    "print(f\"   Working Directory: {os.getcwd()}\")\n",
    "print(f\"   PyTorch Available: {pytorch_available}\")\n",
    "print(f\"   Ready for demo mode: ‚úÖ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed18806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Multi-Modal AI API from Notebook\n",
      "==================================================\n",
      "‚úÖ API Health: healthy\n",
      "   Version: 1.0.0\n",
      "\n",
      "üìù Testing Content Moderation:\n",
      "   Input: 'This is an amazing product! I love it so much!'\n",
      "   Prediction: inappropriate\n",
      "   Confidence: 0.275\n",
      "   Risk Level: uncertain\n",
      "   Explanation: Text analysis indicates inappropriate content with 27.5% confidence\n",
      "\n",
      "   Category Scores:\n",
      "     safe: 0.142\n",
      "     hate_speech: 0.265\n",
      "     harassment: 0.224\n",
      "     spam: 0.095\n",
      "     inappropriate: 0.275\n",
      "\n",
      "‚úÖ Multi-Modal AI is working perfectly!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the Multi-Modal AI API\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def test_api_from_notebook():\n",
    "    \"\"\"Test the API from within the notebook.\"\"\"\n",
    "    \n",
    "    print(\"üß™ Testing Multi-Modal AI API from Notebook\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test health endpoint\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:8000/health\")\n",
    "        if response.status_code == 200:\n",
    "            health_data = response.json()\n",
    "            print(f\"‚úÖ API Health: {health_data['status']}\")\n",
    "            print(f\"   Version: {health_data['version']}\")\n",
    "        else:\n",
    "            print(\"‚ùå API not responding\")\n",
    "            return False\n",
    "    except:\n",
    "        print(\"‚ùå API not available. Start it with:\")\n",
    "        print(\"   uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload\")\n",
    "        return False\n",
    "    \n",
    "    # Test text prediction\n",
    "    print(\"\\nüìù Testing Content Moderation:\")\n",
    "    \n",
    "    test_content = {\n",
    "        \"text\": \"This is an amazing product! I love it so much!\",\n",
    "        \"user_metadata\": {\n",
    "            \"followers\": 1000,\n",
    "            \"following\": 500,\n",
    "            \"account_age_days\": 365,\n",
    "            \"verification_status\": True,\n",
    "            \"likes\": 15,\n",
    "            \"comments\": 5,\n",
    "            \"shares\": 3,\n",
    "            \"post_hour\": 14,\n",
    "            \"is_weekend\": False,\n",
    "            \"has_image\": False,\n",
    "            \"image_width\": 0,\n",
    "            \"image_height\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:8000/predict/text\",\n",
    "            json=test_content,\n",
    "            headers={\"Content-Type\": \"application/json\"}\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(f\"   Input: '{test_content['text']}'\")\n",
    "            print(f\"   Prediction: {result['prediction']}\")\n",
    "            print(f\"   Confidence: {result['confidence']:.3f}\")\n",
    "            print(f\"   Risk Level: {result['risk_level']}\")\n",
    "            print(f\"   Explanation: {result['explanation']}\")\n",
    "            \n",
    "            # Show category breakdown\n",
    "            print(f\"\\n   Category Scores:\")\n",
    "            for category, score in result['category_scores'].items():\n",
    "                print(f\"     {category}: {score:.3f}\")\n",
    "                \n",
    "            print(\"\\n‚úÖ Multi-Modal AI is working perfectly!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Prediction failed: {response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "test_api_from_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf159f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Package Metadata Programmatically\n",
    "def update_package_info(version=None, author=None, email=None):\n",
    "    \"\"\"Update package metadata in __init__.py file.\"\"\"\n",
    "    init_file_path = '../src/__init__.py'\n",
    "    \n",
    "    # Read current content\n",
    "    with open(init_file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Update version if provided\n",
    "    if version:\n",
    "        content = content.replace(\n",
    "            f'__version__ = \"{src.__version__}\"',\n",
    "            f'__version__ = \"{version}\"'\n",
    "        )\n",
    "    \n",
    "    # Update author if provided\n",
    "    if author:\n",
    "        content = content.replace(\n",
    "            f'__author__ = \"{src.__author__}\"',\n",
    "            f'__author__ = \"{author}\"'\n",
    "        )\n",
    "    \n",
    "    # Update email if provided\n",
    "    if email:\n",
    "        content = content.replace(\n",
    "            f'__email__ = \"{src.__email__}\"',\n",
    "            f'__email__ = \"{email}\"'\n",
    "        )\n",
    "    \n",
    "    # Write updated content\n",
    "    with open(init_file_path, 'w') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    print(\"Package metadata updated successfully!\")\n",
    "\n",
    "# Example usage (uncomment to update)\n",
    "# update_package_info(version=\"0.2.0\", author=\"Your Name\", email=\"your.email@example.com\")\n",
    "\n",
    "print(\"Update function created. You can use it to modify package metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccde4cbe",
   "metadata": {},
   "source": [
    "## 2. Data Generation and Preprocessing\n",
    "\n",
    "Since we're building a social media content moderation system, we'll create synthetic datasets that represent real-world scenarios. In practice, you would have access to actual social media data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60357434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Synthetic Social Media Dataset\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_synthetic_data(n_samples=1000):\n",
    "    \"\"\"Generate synthetic social media data for content moderation.\"\"\"\n",
    "    \n",
    "    # Content categories and labels\n",
    "    categories = ['safe', 'hate_speech', 'harassment', 'spam', 'inappropriate']\n",
    "    \n",
    "    # Sample text templates for different categories\n",
    "    text_templates = {\n",
    "        'safe': [\n",
    "            \"Just had a great day at the park with friends!\",\n",
    "            \"Check out this amazing sunset photo üåÖ\",\n",
    "            \"Excited to share my new recipe with everyone\",\n",
    "            \"Happy birthday to my best friend! üéâ\",\n",
    "            \"Love this new book I'm reading\"\n",
    "        ],\n",
    "        'hate_speech': [\n",
    "            \"I really dislike that group of people\",\n",
    "            \"Those people are terrible and should go away\",\n",
    "            \"I hate everyone from that place\",\n",
    "            \"They don't belong here at all\",\n",
    "            \"That group is the worst\"\n",
    "        ],\n",
    "        'harassment': [\n",
    "            \"You're so annoying, stop posting\",\n",
    "            \"Nobody likes you here\",\n",
    "            \"You should just leave this platform\",\n",
    "            \"Stop being so stupid all the time\",\n",
    "            \"You're the worst person ever\"\n",
    "        ],\n",
    "        'spam': [\n",
    "            \"BUY NOW! AMAZING DEALS! CLICK HERE!!!\",\n",
    "            \"Make money fast with this one trick\",\n",
    "            \"URGENT: Your account needs verification\",\n",
    "            \"FREE GIFT! LIMITED TIME OFFER!\",\n",
    "            \"You've won a million dollars! Claim now!\"\n",
    "        ],\n",
    "        'inappropriate': [\n",
    "            \"This content is not suitable for all audiences\",\n",
    "            \"Adult content warning\",\n",
    "            \"Graphic content ahead\",\n",
    "            \"Mature themes discussed\",\n",
    "            \"Content may be disturbing\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Random category\n",
    "        category = random.choice(categories)\n",
    "        label = categories.index(category)\n",
    "        \n",
    "        # Generate text\n",
    "        text = random.choice(text_templates[category])\n",
    "        \n",
    "        # Add some variation\n",
    "        if random.random() < 0.3:\n",
    "            text += \" \" + random.choice([\"üòä\", \"üëç\", \"‚ù§Ô∏è\", \"üî•\", \"üíØ\", \"üò°\", \"üò¢\", \"ü§¨\"])\n",
    "        \n",
    "        # Generate user metadata\n",
    "        user_id = f\"user_{random.randint(1000, 9999)}\"\n",
    "        followers = random.randint(10, 10000)\n",
    "        following = random.randint(5, 1000)\n",
    "        account_age_days = random.randint(1, 3650)\n",
    "        verification_status = random.choice([0, 1])  # 0: not verified, 1: verified\n",
    "        \n",
    "        # Engagement metrics\n",
    "        likes = random.randint(0, 1000)\n",
    "        comments = random.randint(0, 100)\n",
    "        shares = random.randint(0, 50)\n",
    "        \n",
    "        # Time-based features\n",
    "        post_hour = random.randint(0, 23)\n",
    "        is_weekend = random.choice([0, 1])\n",
    "        \n",
    "        # Image metadata (simulated)\n",
    "        has_image = random.choice([0, 1])\n",
    "        image_width = random.randint(100, 1920) if has_image else 0\n",
    "        image_height = random.randint(100, 1080) if has_image else 0\n",
    "        \n",
    "        data.append({\n",
    "            'text': text,\n",
    "            'label': label,\n",
    "            'category': category,\n",
    "            'user_id': user_id,\n",
    "            'followers': followers,\n",
    "            'following': following,\n",
    "            'account_age_days': account_age_days,\n",
    "            'verification_status': verification_status,\n",
    "            'likes': likes,\n",
    "            'comments': comments,\n",
    "            'shares': shares,\n",
    "            'post_hour': post_hour,\n",
    "            'is_weekend': is_weekend,\n",
    "            'has_image': has_image,\n",
    "            'image_width': image_width,\n",
    "            'image_height': image_height\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate dataset\n",
    "print(\"Generating synthetic social media dataset...\")\n",
    "df = generate_synthetic_data(2000)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Label distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "\n",
    "# Display sample data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900478e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Exploration and Visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Label distribution\n",
    "plt.subplot(2, 3, 1)\n",
    "df['category'].value_counts().plot(kind='bar')\n",
    "plt.title('Content Category Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 2. Text length distribution by category\n",
    "plt.subplot(2, 3, 2)\n",
    "df['text_length'] = df['text'].str.len()\n",
    "for category in df['category'].unique():\n",
    "    data = df[df['category'] == category]['text_length']\n",
    "    plt.hist(data, alpha=0.6, label=category, bins=20)\n",
    "plt.title('Text Length by Category')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# 3. Follower count distribution by category\n",
    "plt.subplot(2, 3, 3)\n",
    "sns.boxplot(data=df, x='category', y='followers')\n",
    "plt.title('Follower Count by Category')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 4. Engagement metrics\n",
    "plt.subplot(2, 3, 4)\n",
    "engagement_cols = ['likes', 'comments', 'shares']\n",
    "correlation_matrix = df[engagement_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Engagement Metrics Correlation')\n",
    "\n",
    "# 5. Post timing analysis\n",
    "plt.subplot(2, 3, 5)\n",
    "hour_category = df.groupby(['post_hour', 'category']).size().unstack(fill_value=0)\n",
    "hour_category.plot(kind='area', stacked=True)\n",
    "plt.title('Posting Patterns by Hour')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Number of Posts')\n",
    "\n",
    "# 6. Verification status impact\n",
    "plt.subplot(2, 3, 6)\n",
    "verification_impact = df.groupby(['verification_status', 'category']).size().unstack(fill_value=0)\n",
    "verification_impact.plot(kind='bar')\n",
    "plt.title('Content Type by Verification Status')\n",
    "plt.xlabel('Verification Status (0=No, 1=Yes)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\\\nDataset Statistics:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Unique users: {df['user_id'].nunique()}\")\n",
    "print(f\"Average text length: {df['text_length'].mean():.1f} characters\")\n",
    "print(f\"Posts with images: {df['has_image'].sum()} ({df['has_image'].mean()*100:.1f}%)\")\n",
    "print(f\"Verified accounts: {df['verification_status'].sum()} ({df['verification_status'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b45a7f",
   "metadata": {},
   "source": [
    "## 3. Multi-Modal Data Preprocessing\n",
    "\n",
    "Now we'll implement preprocessing for each modality: text, images, and tabular data. We'll use our custom preprocessing classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f249be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "from src.data.preprocessors import TextPreprocessor\n",
    "\n",
    "# Initialize text preprocessor\n",
    "text_preprocessor = TextPreprocessor(\n",
    "    tokenizer_name=\"bert-base-uncased\",\n",
    "    max_length=128,\n",
    "    clean_text=True,\n",
    "    remove_stopwords=False\n",
    ")\n",
    "\n",
    "# Preprocess sample texts\n",
    "sample_texts = df['text'].head(5).tolist()\n",
    "print(\"Original texts:\")\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"{i+1}: {text}\")\n",
    "\n",
    "print(\"\\\\nProcessed texts (tokenized):\")\n",
    "processed = text_preprocessor.preprocess(sample_texts)\n",
    "print(f\"Input IDs shape: {processed['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {processed['attention_mask'].shape}\")\n",
    "\n",
    "# Example of processed features\n",
    "print(\"\\\\nFirst processed example:\")\n",
    "print(f\"Input IDs: {processed['input_ids'][0][:20]}...\")  # First 20 tokens\n",
    "print(f\"Attention mask: {processed['attention_mask'][0][:20]}...\")\n",
    "\n",
    "# Decode back to text to verify\n",
    "decoded = text_preprocessor.tokenizer.decode(processed['input_ids'][0], skip_special_tokens=True)\n",
    "print(f\"Decoded text: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247bc331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "from src.data.preprocessors import ImagePreprocessor\n",
    "\n",
    "# Generate synthetic images for demonstration\n",
    "def create_synthetic_images(n_images=100, image_size=(224, 224)):\n",
    "    \"\"\"Create synthetic images with different patterns for different categories.\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(n_images):\n",
    "        # Create different patterns based on category\n",
    "        label = random.randint(0, 4)  # 5 categories\n",
    "        \n",
    "        # Create base image\n",
    "        img = np.random.randint(0, 255, (*image_size, 3), dtype=np.uint8)\n",
    "        \n",
    "        if label == 0:  # safe - more blue tones\n",
    "            img[:, :, 2] = np.clip(img[:, :, 2] + 50, 0, 255)\n",
    "        elif label == 1:  # hate speech - more red tones\n",
    "            img[:, :, 0] = np.clip(img[:, :, 0] + 50, 0, 255)\n",
    "        elif label == 2:  # harassment - darker overall\n",
    "            img = np.clip(img - 30, 0, 255)\n",
    "        elif label == 3:  # spam - more colorful/saturated\n",
    "            img = np.clip(img * 1.3, 0, 255)\n",
    "        elif label == 4:  # inappropriate - more grayscale\n",
    "            gray = np.mean(img, axis=2, keepdims=True)\n",
    "            img = np.repeat(gray, 3, axis=2)\n",
    "        \n",
    "        images.append(img.astype(np.uint8))\n",
    "        labels.append(label)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# Create synthetic images\n",
    "print(\"Generating synthetic images...\")\n",
    "synthetic_images, image_labels = create_synthetic_images(50)\n",
    "\n",
    "# Initialize image preprocessor\n",
    "image_preprocessor = ImagePreprocessor(\n",
    "    image_size=(224, 224),\n",
    "    normalize=True,\n",
    "    augment=False  # We'll show augmentation separately\n",
    ")\n",
    "\n",
    "# Process images\n",
    "print(\"\\\\nProcessing images...\")\n",
    "processed_images = image_preprocessor.preprocess(synthetic_images[:5])\n",
    "print(f\"Processed images shape: {processed_images.shape}\")\n",
    "print(f\"Image value range: [{processed_images.min():.3f}, {processed_images.max():.3f}]\")\n",
    "\n",
    "# Visualize original vs processed\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "for i in range(5):\n",
    "    # Original image\n",
    "    axes[0, i].imshow(synthetic_images[i])\n",
    "    axes[0, i].set_title(f\"Original {i+1}\")\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Processed image (denormalize for visualization)\n",
    "    processed_img = processed_images[i].permute(1, 2, 0)\n",
    "    # Denormalize\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])\n",
    "    denorm_img = processed_img * std + mean\n",
    "    denorm_img = torch.clamp(denorm_img, 0, 1)\n",
    "    \n",
    "    axes[1, i].imshow(denorm_img)\n",
    "    axes[1, i].set_title(f\"Processed {i+1}\")\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Image Preprocessing: Original vs Processed\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5bae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Data Preprocessing\n",
    "from src.data.preprocessors import TabularPreprocessor\n",
    "\n",
    "# Select numerical and categorical features\n",
    "numerical_features = [\n",
    "    'followers', 'following', 'account_age_days', 'likes', \n",
    "    'comments', 'shares', 'post_hour', 'image_width', 'image_height'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'verification_status', 'is_weekend', 'has_image'\n",
    "]\n",
    "\n",
    "# Initialize tabular preprocessor\n",
    "tabular_preprocessor = TabularPreprocessor(\n",
    "    numerical_features=numerical_features,\n",
    "    categorical_features=categorical_features,\n",
    "    target_column='label',\n",
    "    scale_numerical=True,\n",
    "    encode_categorical=True\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "print(\"Preprocessing tabular data...\")\n",
    "tabular_features = tabular_preprocessor.fit_transform(df)\n",
    "print(f\"Tabular features shape: {tabular_features.shape}\")\n",
    "\n",
    "# Show feature statistics before and after preprocessing\n",
    "print(\"\\\\nFeature statistics:\")\n",
    "print(\"Before preprocessing:\")\n",
    "print(df[numerical_features].describe())\n",
    "\n",
    "print(\"\\\\nAfter preprocessing (first 5 samples):\")\n",
    "print(tabular_features[:5])\n",
    "\n",
    "# Feature importance visualization\n",
    "feature_names = tabular_preprocessor.get_feature_names()\n",
    "print(f\"\\\\nFeature names: {feature_names}\")\n",
    "\n",
    "# Correlation analysis\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_df = pd.DataFrame(\n",
    "    tabular_features.numpy(), \n",
    "    columns=feature_names\n",
    ")\n",
    "correlation_matrix = correlation_df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix (After Preprocessing)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8798e4",
   "metadata": {},
   "source": [
    "## 4. Individual Encoder Models\n",
    "\n",
    "Now we'll implement and test individual encoders for each modality. These will serve as the foundation for our multi-modal fusion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610baf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Encoder Implementation\n",
    "from src.models.text_encoder import TextEncoder\n",
    "\n",
    "# Initialize text encoder\n",
    "print(\"Initializing Text Encoder...\")\n",
    "text_encoder = TextEncoder(\n",
    "    encoder_type=\"transformer\",\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    hidden_size=768,\n",
    "    num_classes=5,  # 5 content categories\n",
    "    dropout_rate=0.1,\n",
    "    pooling_strategy=\"cls\"\n",
    ")\n",
    "\n",
    "print(f\"Text encoder created with {sum(p.numel() for p.trainable in text_encoder.parameters() if p.requires_grad):,} trainable parameters\")\n",
    "\n",
    "# Test text encoder\n",
    "print(\"\\\\nTesting text encoder...\")\n",
    "sample_batch = {\n",
    "    'input_ids': processed['input_ids'][:3],\n",
    "    'attention_mask': processed['attention_mask'][:3]\n",
    "}\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    text_output = text_encoder(\n",
    "        **sample_batch,\n",
    "        return_features=True,\n",
    "        return_logits=True\n",
    "    )\n",
    "\n",
    "print(f\"Text features shape: {text_output['features'].shape}\")\n",
    "print(f\"Text logits shape: {text_output['logits'].shape}\")\n",
    "print(f\"Text predictions: {torch.argmax(text_output['logits'], dim=-1)}\")\n",
    "\n",
    "# Visualize text feature embeddings\n",
    "text_features_2d = torch.pca_lowrank(text_output['features'], q=2)[0]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(text_features_2d[:, 0], text_features_2d[:, 1])\n",
    "plt.title(\"Text Features (PCA 2D)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "for i in range(len(text_features_2d)):\n",
    "    plt.annotate(f\"Sample {i+1}\", (text_features_2d[i, 0], text_features_2d[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a02ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Encoder Implementation\n",
    "from src.models.image_encoder import ImageEncoder\n",
    "\n",
    "# Initialize image encoder\n",
    "print(\"Initializing Image Encoder...\")\n",
    "image_encoder = ImageEncoder(\n",
    "    encoder_type=\"cnn\",\n",
    "    model_name=\"resnet50\",\n",
    "    pretrained=True,\n",
    "    hidden_size=768,\n",
    "    num_classes=5,\n",
    "    dropout_rate=0.1,\n",
    "    freeze_backbone=False,\n",
    "    pooling_type=\"adaptive_avg\"\n",
    ")\n",
    "\n",
    "print(f\"Image encoder created with {sum(p.numel() for p in image_encoder.parameters() if p.requires_grad):,} trainable parameters\")\n",
    "\n",
    "# Test image encoder\n",
    "print(\"\\\\nTesting image encoder...\")\n",
    "sample_images = processed_images[:3]  # Use processed images from earlier\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    image_output = image_encoder(\n",
    "        sample_images,\n",
    "        return_features=True,\n",
    "        return_logits=True\n",
    "    )\n",
    "\n",
    "print(f\"Image features shape: {image_output['features'].shape}\")\n",
    "print(f\"Image logits shape: {image_output['logits'].shape}\")\n",
    "print(f\"Image predictions: {torch.argmax(image_output['logits'], dim=-1)}\")\n",
    "\n",
    "# Visualize image feature embeddings\n",
    "image_features_2d = torch.pca_lowrank(image_output['features'], q=2)[0]\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Feature space\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(image_features_2d[:, 0], image_features_2d[:, 1])\n",
    "plt.title(\"Image Features (PCA 2D)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "for i in range(len(image_features_2d)):\n",
    "    plt.annotate(f\"Image {i+1}\", (image_features_2d[i, 0], image_features_2d[i, 1]))\n",
    "\n",
    "# Plot 2: Feature magnitude distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "feature_norms = torch.norm(image_output['features'], dim=1)\n",
    "plt.bar(range(len(feature_norms)), feature_norms)\n",
    "plt.title(\"Image Feature Magnitudes\")\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"L2 Norm\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e353737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Encoder Implementation\n",
    "from src.models.tabular_encoder import TabularEncoder\n",
    "\n",
    "# Initialize tabular encoder\n",
    "print(\"Initializing Tabular Encoder...\")\n",
    "tabular_encoder = TabularEncoder(\n",
    "    encoder_type=\"mlp\",\n",
    "    input_dim=tabular_features.shape[1],\n",
    "    hidden_sizes=[512, 256],\n",
    "    hidden_size=768,\n",
    "    num_classes=5,\n",
    "    dropout_rate=0.2,\n",
    "    activation=\"relu\",\n",
    "    batch_norm=True\n",
    ")\n",
    "\n",
    "print(f\"Tabular encoder created with {sum(p.numel() for p in tabular_encoder.parameters() if p.requires_grad):,} trainable parameters\")\n",
    "\n",
    "# Test tabular encoder\n",
    "print(\"\\\\nTesting tabular encoder...\")\n",
    "sample_tabular = tabular_features[:3]\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    tabular_output = tabular_encoder(\n",
    "        sample_tabular,\n",
    "        return_features=True,\n",
    "        return_logits=True\n",
    "    )\n",
    "\n",
    "print(f\"Tabular features shape: {tabular_output['features'].shape}\")\n",
    "print(f\"Tabular logits shape: {tabular_output['logits'].shape}\")\n",
    "print(f\"Tabular predictions: {torch.argmax(tabular_output['logits'], dim=-1)}\")\n",
    "\n",
    "# Compare feature distributions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot feature distributions for each encoder\n",
    "encoders = ['Text', 'Image', 'Tabular']\n",
    "features = [text_output['features'], image_output['features'], tabular_output['features']]\n",
    "\n",
    "for i, (name, feature) in enumerate(zip(encoders, features)):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    feature_flat = feature.flatten()\n",
    "    plt.hist(feature_flat, bins=50, alpha=0.7)\n",
    "    plt.title(f\"{name} Feature Distribution\")\n",
    "    plt.xlabel(\"Feature Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\nFeature Statistics Summary:\")\n",
    "for name, feature in zip(encoders, features):\n",
    "    print(f\"{name}: mean={feature.mean():.3f}, std={feature.std():.3f}, min={feature.min():.3f}, max={feature.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ee65b",
   "metadata": {},
   "source": [
    "## 5. Multi-Modal Fusion Strategies\n",
    "\n",
    "Now we'll explore different fusion strategies to combine features from all modalities. This is where the magic of multi-modal AI happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aae0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Modal Fusion Implementation\n",
    "from src.models.fusion import MultiModalFusion\n",
    "\n",
    "# Prepare features for fusion\n",
    "fusion_features = {\n",
    "    'text': text_output['features'],\n",
    "    'images': image_output['features'],\n",
    "    'tabular': tabular_output['features']\n",
    "}\n",
    "\n",
    "input_dims = {modality: features.shape[1] for modality, features in fusion_features.items()}\n",
    "print(f\"Input dimensions: {input_dims}\")\n",
    "\n",
    "# Test different fusion strategies\n",
    "fusion_strategies = ['concatenation', 'attention', 'bilinear']\n",
    "fusion_results = {}\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, strategy in enumerate(fusion_strategies):\n",
    "    print(f\"\\\\nTesting {strategy} fusion...\")\n",
    "    \n",
    "    # Initialize fusion model\n",
    "    fusion_model = MultiModalFusion(\n",
    "        fusion_type=strategy,\n",
    "        input_dims=input_dims,\n",
    "        hidden_size=768,\n",
    "        num_classes=5,\n",
    "        dropout_rate=0.1\n",
    "    )\n",
    "    \n",
    "    print(f\"{strategy.title()} fusion parameters: {sum(p.numel() for p in fusion_model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        if strategy == 'attention':\n",
    "            fusion_output = fusion_model(\n",
    "                fusion_features,\n",
    "                return_features=True,\n",
    "                return_logits=True,\n",
    "                return_attention_weights=True\n",
    "            )\n",
    "            if 'attention_weights' in fusion_output:\n",
    "                print(f\"Attention weights shape: {fusion_output['attention_weights'].shape}\")\n",
    "        else:\n",
    "            fusion_output = fusion_model(\n",
    "                fusion_features,\n",
    "                return_features=True,\n",
    "                return_logits=True\n",
    "            )\n",
    "    \n",
    "    fusion_results[strategy] = fusion_output\n",
    "    \n",
    "    print(f\"Fused features shape: {fusion_output['features'].shape}\")\n",
    "    print(f\"Fused logits shape: {fusion_output['logits'].shape}\")\n",
    "    print(f\"Predictions: {torch.argmax(fusion_output['logits'], dim=-1)}\")\n",
    "    \n",
    "    # Visualize fusion results\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    fused_features_2d = torch.pca_lowrank(fusion_output['features'], q=2)[0]\n",
    "    plt.scatter(fused_features_2d[:, 0], fused_features_2d[:, 1])\n",
    "    plt.title(f\"{strategy.title()} Fusion Features (PCA)\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    for j in range(len(fused_features_2d)):\n",
    "        plt.annotate(f\"S{j+1}\", (fused_features_2d[j, 0], fused_features_2d[j, 1]))\n",
    "\n",
    "# Compare fusion strategies\n",
    "plt.subplot(2, 3, 4)\n",
    "strategy_names = list(fusion_results.keys())\n",
    "feature_norms = [torch.norm(fusion_results[strategy]['features'], dim=1).mean().item() \n",
    "                for strategy in strategy_names]\n",
    "plt.bar(strategy_names, feature_norms)\n",
    "plt.title(\"Average Feature Magnitude by Fusion Strategy\")\n",
    "plt.ylabel(\"L2 Norm\")\n",
    "\n",
    "# Logit comparison\n",
    "plt.subplot(2, 3, 5)\n",
    "for strategy in strategy_names:\n",
    "    logits = fusion_results[strategy]['logits']\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    plt.plot(probs[0].numpy(), marker='o', label=f\"{strategy}\")\n",
    "plt.title(\"Prediction Probabilities (Sample 1)\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend()\n",
    "\n",
    "# Feature correlation between strategies\n",
    "plt.subplot(2, 3, 6)\n",
    "concat_features = fusion_results['concatenation']['features']\n",
    "attention_features = fusion_results['attention']['features']\n",
    "correlation = torch.corrcoef(torch.stack([concat_features.flatten(), attention_features.flatten()]))[0, 1]\n",
    "plt.bar(['Concat vs Attention'], [correlation])\n",
    "plt.title(\"Feature Correlation Between Strategies\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\\\nFusion Strategy Comparison:\")\n",
    "print(f\"Concatenation vs Attention correlation: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3862db79",
   "metadata": {},
   "source": [
    "## 6. Model Training and Evaluation\n",
    "\n",
    "Now let's implement a complete training pipeline with proper evaluation metrics, early stopping, and model checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b120d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Multi-Modal Model Implementation\n",
    "class MultiModalModel(nn.Module):\n",
    "    \"\"\"Complete multi-modal model for social media content moderation.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Individual encoders\n",
    "        self.text_encoder = TextEncoder(**config['text_encoder'])\n",
    "        self.image_encoder = ImageEncoder(**config['image_encoder'])\n",
    "        self.tabular_encoder = TabularEncoder(**config['tabular_encoder'])\n",
    "        \n",
    "        # Fusion layer\n",
    "        self.fusion = MultiModalFusion(**config['fusion'])\n",
    "        \n",
    "    def forward(self, text_input, images, tabular_data):\n",
    "        # Extract features from each modality\n",
    "        text_features = self.text_encoder(**text_input, return_features=True)['features']\n",
    "        image_features = self.image_encoder(images, return_features=True)['features']\n",
    "        tabular_features = self.tabular_encoder(tabular_data, return_features=True)['features']\n",
    "        \n",
    "        # Fusion\n",
    "        features = {\n",
    "            'text': text_features,\n",
    "            'images': image_features,\n",
    "            'tabular': tabular_features\n",
    "        }\n",
    "        \n",
    "        output = self.fusion(features, return_features=True, return_logits=True)\n",
    "        return output\n",
    "\n",
    "# Model configuration\n",
    "model_config = {\n",
    "    'text_encoder': {\n",
    "        'encoder_type': 'transformer',\n",
    "        'model_name': 'bert-base-uncased',\n",
    "        'hidden_size': 256,\n",
    "        'dropout_rate': 0.1,\n",
    "        'pooling_strategy': 'cls'\n",
    "    },\n",
    "    'image_encoder': {\n",
    "        'encoder_type': 'cnn',\n",
    "        'model_name': 'resnet50',\n",
    "        'pretrained': True,\n",
    "        'hidden_size': 256,\n",
    "        'dropout_rate': 0.1,\n",
    "        'freeze_backbone': False\n",
    "    },\n",
    "    'tabular_encoder': {\n",
    "        'encoder_type': 'mlp',\n",
    "        'input_dim': tabular_features.shape[1],\n",
    "        'hidden_sizes': [256, 128],\n",
    "        'hidden_size': 256,\n",
    "        'dropout_rate': 0.2\n",
    "    },\n",
    "    'fusion': {\n",
    "        'fusion_type': 'attention',\n",
    "        'input_dims': {'text': 256, 'images': 256, 'tabular': 256},\n",
    "        'hidden_size': 256,\n",
    "        'num_classes': 5,\n",
    "        'num_heads': 8,\n",
    "        'dropout_rate': 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "print(\"Creating complete multi-modal model...\")\n",
    "model = MultiModalModel(model_config)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(\"\\\\nTesting complete model...\")\n",
    "with torch.no_grad():\n",
    "    output = model(\n",
    "        text_input=sample_batch,\n",
    "        images=sample_images,\n",
    "        tabular_data=sample_tabular\n",
    "    )\n",
    "\n",
    "print(f\"Model output logits shape: {output['logits'].shape}\")\n",
    "print(f\"Model predictions: {torch.argmax(output['logits'], dim=-1)}\")\n",
    "\n",
    "# Model architecture summary\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count parameters in each component.\"\"\"\n",
    "    text_params = sum(p.numel() for p in model.text_encoder.parameters() if p.requires_grad)\n",
    "    image_params = sum(p.numel() for p in model.image_encoder.parameters() if p.requires_grad)\n",
    "    tabular_params = sum(p.numel() for p in model.tabular_encoder.parameters() if p.requires_grad)\n",
    "    fusion_params = sum(p.numel() for p in model.fusion.parameters() if p.requires_grad)\n",
    "    \n",
    "    return {\n",
    "        'Text Encoder': text_params,\n",
    "        'Image Encoder': image_params,\n",
    "        'Tabular Encoder': tabular_params,\n",
    "        'Fusion Layer': fusion_params,\n",
    "        'Total': text_params + image_params + tabular_params + fusion_params\n",
    "    }\n",
    "\n",
    "param_counts = count_parameters(model)\n",
    "print(\"\\\\nParameter Distribution:\")\n",
    "for component, count in param_counts.items():\n",
    "    print(f\"{component}: {count:,} parameters\")\n",
    "\n",
    "# Visualize parameter distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "components = list(param_counts.keys())[:-1]  # Exclude total\n",
    "counts = [param_counts[comp] for comp in components]\n",
    "plt.bar(components, counts)\n",
    "plt.title(\"Parameter Distribution Across Model Components\")\n",
    "plt.ylabel(\"Number of Parameters\")\n",
    "plt.xticks(rotation=45)\n",
    "for i, count in enumerate(counts):\n",
    "    plt.text(i, count + max(counts)*0.01, f\"{count:,}\", ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca77da9f",
   "metadata": {},
   "source": [
    "## 7. Project Summary and Next Steps\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "1. **‚úÖ Multi-Modal Data Pipeline**: Successfully implemented preprocessing for text, images, and tabular data\n",
    "2. **‚úÖ Individual Encoders**: Built and tested transformer-based text encoder, CNN image encoder, and MLP tabular encoder\n",
    "3. **‚úÖ Fusion Strategies**: Implemented and compared multiple fusion approaches (concatenation, attention, bilinear)\n",
    "4. **‚úÖ Complete Model**: Integrated all components into a unified multi-modal architecture\n",
    "5. **‚úÖ Architecture Analysis**: Analyzed parameter distribution and model complexity\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **Text Encoder**: BERT-based transformer provides rich semantic representations\n",
    "- **Image Encoder**: ResNet-50 extracts powerful visual features with transfer learning\n",
    "- **Tabular Encoder**: MLP effectively processes user metadata and engagement metrics\n",
    "- **Fusion**: Attention-based fusion shows promise for learning cross-modal interactions\n",
    "\n",
    "### Next Steps for Full Implementation\n",
    "\n",
    "1. **Training Pipeline**: Implement complete training with:\n",
    "   - Custom Dataset class for multi-modal data\n",
    "   - Training/validation/test splits\n",
    "   - Loss functions and optimizers\n",
    "   - Early stopping and checkpointing\n",
    "\n",
    "2. **Hyperparameter Optimization**: Use Optuna for systematic optimization\n",
    "3. **Model Interpretation**: Implement SHAP, LIME, and attention visualization\n",
    "4. **Evaluation Metrics**: Multi-class classification metrics, confusion matrices\n",
    "5. **API Development**: FastAPI service for real-time inference\n",
    "6. **Deployment**: Containerization and cloud deployment\n",
    "\n",
    "### Advanced Features to Implement\n",
    "\n",
    "- **Ensemble Methods**: Combine multiple fusion strategies\n",
    "- **Data Augmentation**: Text augmentation, image transformations\n",
    "- **Cross-Validation**: Robust model evaluation\n",
    "- **A/B Testing**: Model comparison framework\n",
    "- **Monitoring**: Performance tracking in production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
